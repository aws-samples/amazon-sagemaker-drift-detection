{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Taxi Fare Prediction\n",
    "\n",
    "Predict taxi fares using the [New York City Taxi and Limousine Commission (TLC) Trip Record Data](https://registry.opendata.aws/nyc-tlc-trip-records-pds/) public dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U pandas geopandas seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    " \n",
    "In this section of the notebook, you will download the publicly available New York Taxi dataset in preparation for uploading it to S3.\n",
    "\n",
    "### Download Dataset\n",
    "\n",
    "First, download a sample of the New York City Taxi [dataset](https://registry.opendata.aws/nyc-tlc-trip-records-pds/)⇗ to this notebook instance. \n",
    "\n",
    "This dataset contains information on trips taken by taxis and for-hire vehicles in New York City, including pick-up and drop-off times and locations, fares, distance traveled, and more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp 's3://nyc-tlc/trip data/green_tripdata_2018-02.csv' 'nyc-tlc.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp 's3://nyc-tlc/misc/taxi_zones.zip' 'taxi_zones.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip taxi_zones.zip -d shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets\n",
    "\n",
    "Load the trip dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "trip_df = pd.read_csv(\n",
    "    \"nyc-tlc.csv\", parse_dates=[\"lpep_pickup_datetime\", \"lpep_dropoff_datetime\"]\n",
    ")\n",
    "trip_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the taxi zone shape data to get the gemotry and calculate a centroid and lat/long  each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Load the shape file and get the geometry and lat/lon\n",
    "zones = gpd.read_file(\"shapes/taxi_zones.shp\")\n",
    "# Return Centroid as CRS code of 3310 for calcuating distance in meters.\n",
    "zones[\"centroid\"] = zones.geometry.centroid.to_crs(epsg=3310)\n",
    "# Convert cordinates to the WSG84 lat/long CRS has a EPSG code of 4326.\n",
    "zones[\"latitude\"] = zones.centroid.to_crs(epsg=4326).x\n",
    "zones[\"longitude\"] = zones.centroid.to_crs(epsg=4326).y\n",
    "\n",
    "# Drop duplicate by location ID keeping the first\n",
    "zones = zones.drop_duplicates(subset=\"LocationID\", keep=\"first\")\n",
    "# Drop cols we don't need and inspect results\n",
    "zones = zones.set_index(\"LocationID\").drop(\n",
    "    [\"OBJECTID\", \"Shape_Leng\", \"Shape_Area\"], axis=1\n",
    ")\n",
    "zones.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the trip data to the zone and calculate the distance between centroids (should take < 20 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trip_df = gpd.GeoDataFrame(\n",
    "    trip_df.join(zones, on=\"PULocationID\").join(\n",
    "        zones, on=\"DOLocationID\", rsuffix=\"_DO\", lsuffix=\"_PU\"\n",
    "    )\n",
    ")\n",
    "trip_df[\"geo_distance\"] = trip_df[\"centroid_PU\"].distance(trip_df[\"centroid_DO\"]) / 1000\n",
    "trip_df[[\"PULocationID\", \"DOLocationID\", \"trip_distance\", \"geo_distance\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add datetime parts based on pickup time and duration to validate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_df[\"hour\"] = trip_df.lpep_pickup_datetime.dt.hour\n",
    "trip_df[\"weekday\"] = trip_df.lpep_pickup_datetime.dt.weekday\n",
    "trip_df[\"month\"] = trip_df.lpep_pickup_datetime.dt.month\n",
    "trip_df[\"duration_minutes\"] = (\n",
    "    trip_df[\"lpep_dropoff_datetime\"] - trip_df[\"lpep_pickup_datetime\"]\n",
    ").dt.seconds / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization\n",
    "\n",
    "Let's check that we have a good spread of travel across each day of the week and hour of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(trip_df, x=\"hour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot that we have a distribution across week days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(trip_df, x=\"weekday\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's validate that the geo distance correlations generally with the fare amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = trip_df[trip_df[\"geo_distance\"] > 0].sample(1000)\n",
    "sns.jointplot(data=sample_df, x=\"geo_distance\", y=\"fare_amount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the geometry of the map along with centroids for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "\n",
    "def annotate(ax, z):\n",
    "    txt = f\"{z.name}: {z.zone} ({-z.latitude:.2f}°N, {z.longitude:.2f}°W)\"\n",
    "    ax.annotate(txt, (z.latitude, z.longitude))\n",
    "\n",
    "\n",
    "def arrow(ax, ll):\n",
    "    ld = ll.iloc[1] - ll.iloc[0]\n",
    "    ax.arrow(\n",
    "        ll.iloc[0].latitude,\n",
    "        ll.iloc[0].longitude,\n",
    "        ld.latitude,\n",
    "        ld.longitude,\n",
    "        length_includes_head=True,\n",
    "        edgecolor=\"lightgrey\",\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_map(zones, zids):\n",
    "    # Render the geometry in Lat/Lon space\n",
    "    ax = zones.geometry.to_crs(epsg=4326).plot(\n",
    "        figsize=(15, 15), color=\"whitesmoke\", edgecolor=\"lightgrey\", linewidth=0.5\n",
    "    )\n",
    "    # Draw arrow\n",
    "    arrow(ax, zones.loc[zids][[\"latitude\", \"longitude\"]])\n",
    "    # Plot centroid\n",
    "    centroids = zones.loc[zids].geometry.centroid.to_crs(\n",
    "        epsg=3310\n",
    "    )  # Require this format for calculating distance\n",
    "    centroids.to_crs(epsg=4326).plot(ax=ax, color=\"red\", marker=\"+\")\n",
    "    # Annotate points\n",
    "    for i, row in zones.loc[zids].iterrows():\n",
    "        annotate(ax, row)\n",
    "    # Output the distance traveled\n",
    "    dist = centroids.iloc[0].distance(centroids.iloc[1]) / 1000\n",
    "    plt.title(f\"From zone {zids[0]} to {zids[1]} distance: {dist:.2f}km\")\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a trip to inspect the zones it travels from and to and the duration and cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_idx = 5\n",
    "\n",
    "# Get the trip and plot on map\n",
    "t = trip_df.iloc[trip_idx]\n",
    "dist = plot_map(zones, [t.PULocationID, t.DOLocationID])\n",
    "\n",
    "print(\n",
    "    f\"Took {t.duration_minutes:.2f} minutes on {t.weekday} at {t.hour} hour to travel {dist:.2f}km for the cost of ${t.fare_amount:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename and select columns that we want build model on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename cols\n",
    "trip_df = trip_df.rename(\n",
    "    columns={\n",
    "        \"latitude_PU\": \"pickup_latitude\",\n",
    "        \"longitude_PU\": \"pickup_longitude\",\n",
    "        \"latitude_DO\": \"dropoff_latitude\",\n",
    "        \"longitude_DO\": \"dropoff_longitude\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Select cols\n",
    "cols = [\n",
    "    \"fare_amount\",\n",
    "    \"pickup_latitude\",\n",
    "    \"pickup_longitude\",\n",
    "    \"dropoff_latitude\",\n",
    "    \"dropoff_longitude\",\n",
    "    \"geo_distance\",\n",
    "    \"hour\",\n",
    "    \"weekday\",\n",
    "    \"month\",\n",
    "]\n",
    "data_df = trip_df[cols]\n",
    "data_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up to remove some outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df[\n",
    "    (data_df.fare_amount > 0)\n",
    "    & (data_df.fare_amount < 200)\n",
    "    & (data_df.geo_distance >= 0)\n",
    "    & (data_df.geo_distance < 121)\n",
    "].dropna()\n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting and saving\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to split the dataset into train, validation, and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(data_df, test_size=0.20, random_state=42)\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.05, random_state=42)\n",
    "\n",
    "# Reset the index for our test dataframe\n",
    "test_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(\n",
    "    \"Size of\\n train: {},\\n val: {},\\n test: {} \".format(\n",
    "        train_df.shape[0], val_df.shape[0], test_df.shape[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the train, validation, and test files as CSV locally on this notebook instance. Notice that you save the train file twice - once as the training data file and once as the baseline data file. The baseline data file will be used by [SageMaker Model Monitor](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html)⇗ to detect data drift. Data drift occurs when the statistical nature of the data that your model receives while in production drifts away from the nature of the baseline data it was trained on, which means the model begins to lose accuracy in its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"train.csv\", index=False, header=False)\n",
    "val_df.to_csv(\"validation.csv\", index=False, header=False)\n",
    "test_df.to_csv(\"test.csv\", index=False, header=False)\n",
    "\n",
    "# Save test and baseline with headers\n",
    "train_df.to_csv(\"baseline.csv\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now upload these CSV files to your default SageMaker S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Get the session and default bucket\n",
    "session = sagemaker.session.Session()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "# Specify data prefix and version\n",
    "prefix = \"nyc-tlc/v2\"\n",
    "\n",
    "s3_train_uri = session.upload_data(\"train.csv\", bucket, prefix + \"/data/training\")\n",
    "s3_val_uri = session.upload_data(\"validation.csv\", bucket, prefix + \"/data/validation\")\n",
    "s3_test_uri = session.upload_data(\"test.csv\", bucket, prefix + \"/data/test\")\n",
    "s3_baseline_uri = session.upload_data(\"baseline.csv\", bucket, prefix + \"/data/baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Job\n",
    "\n",
    "Build an estimator to train on this, see if using geo_distance its okay predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Can XGBoost report use a version which accepts the header for feature importance?\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.debugger import Rule, rule_configs\n",
    "\n",
    "# Get role and region\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sagemaker.session.Session().boto_session.region_name\n",
    "\n",
    "# Define the XGBoost training report rules\n",
    "# see: https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-training-xgboost-report.html\n",
    "rules = [Rule.sagemaker(rule_configs.create_xgboost_report())]\n",
    "\n",
    "# Get the training instance type\n",
    "training_instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "# training step for generating model artifacts\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.2-2\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "\n",
    "output_path = \"s3://{}/{}/output\".format(bucket, prefix)\n",
    "\n",
    "estimator = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    output_path=output_path,\n",
    "    role=role,\n",
    "    disable_profiler=True,  # Profile processing job\n",
    "    rules=rules,  # Report processing job\n",
    ")\n",
    "\n",
    "hp = {\n",
    "    \"max_depth\": \"9\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"300\",\n",
    "    \"subsample\": \"0.8\",\n",
    "    \"objective\": \"reg:squarederror\",  # reg:linear not supported\n",
    "    \"early_stopping_rounds\": \"10\",\n",
    "    \"num_round\": \"100\",\n",
    "}\n",
    "estimator.set_hyperparameters(**hp)\n",
    "\n",
    "# Set the data\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=s3_train_uri, content_type=\"text/csv\"\n",
    ")\n",
    "s3_input_val = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=s3_val_uri, content_type=\"text/csv\"\n",
    ")\n",
    "data = {\"train\": s3_input_train, \"validation\": s3_input_val}\n",
    "\n",
    "estimator.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate \n",
    "\n",
    "Wait for the XGBoost report to be ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client = sagemaker.session.Session().sagemaker_client\n",
    "\n",
    "# Attach the job and get report\n",
    "xgb_report_job_name = [\n",
    "    rule[\"RuleEvaluationJobArn\"].split(\"/\")[-1]\n",
    "    for rule in estimator.latest_training_job.rule_job_summary()\n",
    "    if \"CreateXgboostReport\" in rule[\"RuleConfigurationName\"]\n",
    "][0]\n",
    "\n",
    "print(f\"Waiting for XGBoost training report {xgb_report_job_name} to complete...\")\n",
    "sm_client.get_waiter(\"processing_job_completed_or_stopped\").wait(\n",
    "    ProcessingJobName=xgb_report_job_name\n",
    ")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspects the results of the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "\n",
    "# Get the s3 output\n",
    "report_uri = sm_client.describe_processing_job(ProcessingJobName=xgb_report_job_name)[\n",
    "    \"ProcessingOutputConfig\"\n",
    "][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "\n",
    "# Download the notebook from the report\n",
    "S3Downloader().download(f\"{report_uri}/xgboost_report.html\", \"report\")\n",
    "FileLink(\"report/xgboost_report.html\", result_html_prefix=\"Open Report: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy\n",
    "\n",
    "Deploy an endpoint for the predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    serializer=CSVSerializer(),\n",
    "    deserializer=CSVDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions for the held out test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(seq, batch_size):\n",
    "    return (seq[pos : pos + batch_size] for pos in range(0, len(seq), batch_size))\n",
    "\n",
    "\n",
    "# Make predictions without the first colunns\n",
    "results = []\n",
    "for df in chunker(test_df[test_df.columns[1:]], 20):\n",
    "    results += predictor.predict(data=df.to_csv(index=False, header=False))[0]\n",
    "\n",
    "# Get the fare amoiunt pred back in the dataframe\\\n",
    "predictions = pd.Series(results).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the predictions back to the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({\"fare_amount_prediction\": predictions})\n",
    "pred_df = test_df.join(pred_df)\n",
    "\n",
    "# Get abs error\n",
    "pred_df[\"error\"] = abs(pred_df[\"fare_amount\"] - pred_df[\"fare_amount_prediction\"])\n",
    "pred_df.sort_values(\"error\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the root mean squre error (RMSE) to evaluate the performance of this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def rmse(pred_df):\n",
    "    return sqrt(\n",
    "        mean_squared_error(pred_df[\"fare_amount\"], pred_df[\"fare_amount_prediction\"])\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"RMSE: {}\".format(rmse(pred_df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the residules to see where the errors are relative to the fare amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.residplot(\n",
    "    x=pred_df[\"fare_amount\"], y=pred_df[\"fare_amount_prediction\"], lowess=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07c1d6c68b7b22b50965762993b154aa5a1dd6aa65a365988d7d4c27c573599b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
