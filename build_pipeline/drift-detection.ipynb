{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Drift Detection  Notebook\n",
    "\n",
    "This notebook will exercise the drift detection MLOps pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%capture\n",
    "!pip install -U pandas"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "ðŸ‘‡ Set the project name for your drift pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "project_name = \"<<project_name>>\"  # << Update this drift detection project"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get back the project id and region"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sagemaker\n",
    "import json\n",
    "\n",
    "sess = sagemaker.session.Session()\n",
    "region_name = sess._region_name\n",
    "sm_client = sess.sagemaker_client\n",
    "project_id = sm_client.describe_project(ProjectName=project_name)[\"ProjectId\"]\n",
    "\n",
    "print(f\"Project: {project_name} ({project_id})\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Prep\n",
    "\n",
    "Let's copy some trip data and taxi zone files to the input location"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "\n",
    "# Download trip data and taxi zones to input folder\n",
    "download_uri = \"s3://nyc-tlc/trip data/green_tripdata_2018-02.csv\"\n",
    "S3Downloader().download(download_uri, \"input/data\")\n",
    "download_uri = \"s3://nyc-tlc/misc/taxi_zones.zip\"\n",
    "S3Downloader().download(download_uri, \"input/zones\")\n",
    "\n",
    "# Upload input to the target location\n",
    "artifact_bucket = f\"sagemaker-project-{project_id}-{region_name}\"\n",
    "input_data_uri = f\"s3://{artifact_bucket}/{project_id}/input\"\n",
    "S3Uploader().upload(\"input\", input_data_uri)\n",
    "\n",
    "print(\"Listing input files:\")\n",
    "for s3_uri in S3Downloader.list(input_data_uri):\n",
    "    print(s3_uri)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train\n",
    "\n",
    "Start the pipeline now that we have uploaded some data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"{project_name}-pipeline\"\n",
    "pipeline = Pipeline(pipeline_name)\n",
    "\n",
    "# Start pipeline\n",
    "execution = pipeline.start()\n",
    "execution_name = execution.arn.split(\"/\")[-1]\n",
    "\n",
    "print(f\"Waiting for execution: {execution_name} for pipeline {pipeline_name}...\")\n",
    "execution.wait()\n",
    "execution_status = execution.describe()[\"PipelineExecutionStatus\"]\n",
    "print(f\"Status: {execution_status}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "List the execution steps.  Note that we have baseline and training jobs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for step in execution.list_steps():\n",
    "    print(\"Step: {}, Status: {}\".format(step[\"StepName\"], step[\"StepStatus\"]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate\n",
    "\n",
    "Get the estimator for the training job in the pipeline."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "\n",
    "def get_execution_step(step_name):\n",
    "    return [\n",
    "        step[\"Metadata\"]\n",
    "        for step in execution.list_steps()\n",
    "        if step[\"StepName\"] == step_name\n",
    "    ]\n",
    "\n",
    "\n",
    "training_job_arn = get_execution_step(\"TrainModel\")[0][\"TrainingJob\"][\"Arn\"]\n",
    "training_job_name = training_job_arn.split(\"/\")[-1]\n",
    "estimator = Estimator.attach(training_job_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Download the Debugger XGBoost training report\n",
    "\n",
    "SageMaker Debugger generates a [XGBoost Training Report](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-training-xgboost-report.html) by a processing jobs that run concurrent to the training job. Let's wait for it to complete."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get name of the xgboost training report\n",
    "xgb_report_job_name = [\n",
    "    rule[\"RuleEvaluationJobArn\"].split(\"/\")[-1]\n",
    "    for rule in estimator.latest_training_job.rule_job_summary()\n",
    "    if \"CreateXgboostReport\" in rule[\"RuleConfigurationName\"]\n",
    "][0]\n",
    "\n",
    "print(\"Waiting for XGBoost training report to complete...\")\n",
    "sm_client.get_waiter(\"processing_job_completed_or_stopped\").wait(\n",
    "    ProcessingJobName=xgb_report_job_name\n",
    ")\n",
    "print(\"Done\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "â„¹ï¸ The code below will download the output from the Debugger report in the `report` folder.  Click the link to open the report."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from IPython.display import FileLink\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "\n",
    "# Get the s3 output\n",
    "report_uri = sm_client.describe_processing_job(ProcessingJobName=xgb_report_job_name)[\n",
    "    \"ProcessingOutputConfig\"\n",
    "][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "\n",
    "# Download the notebook from the report\n",
    "S3Downloader().download(f\"{report_uri}/xgboost_report.html\", \"report\")\n",
    "FileLink(\"report/xgboost_report.html\", result_html_prefix=\"Open Report: \")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Approve Model\n",
    "\n",
    "ðŸ›‘ Once we are happy with this training job, we can [Update the Approval Status](https://docs.aws.amazon.com/sagemaker/latest/dg/model-registry-approve.html) of a model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_package_arn = get_execution_step(\"RegisterModel\")[0][\"RegisterModel\"][\"Arn\"]\n",
    "model_package_version = model_package_arn.split(\"/\")[-1]\n",
    "print(f\"Model version: {model_package_version}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's update the status to approved"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_package_update_input_dict = {\n",
    "    \"ModelPackageArn\": model_package_arn,\n",
    "    \"ModelApprovalStatus\": \"Approved\",\n",
    "}\n",
    "model_package_update_response = sm_client.update_model_package(\n",
    "    **model_package_update_input_dict\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deploy\n",
    "\n",
    "Now that our model is approve, the deployment pipeline will kick off shortly."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from botocore.exceptions import WaiterError\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Define the predictor for staging\n",
    "def wait_for_predictor(stage_name):\n",
    "    try:\n",
    "        endpoint_name = f\"sagemaker-{project_name}-{stage_name}\"\n",
    "        predictor = Predictor(\n",
    "            endpoint_name, serializer=CSVSerializer(), deserializer=JSONDeserializer()\n",
    "        )\n",
    "        print(\n",
    "            f\"Waiting for {stage_name} endpoint: {predictor.endpoint_name} to be deployed...\"\n",
    "        )\n",
    "        sm_client.get_waiter(\"endpoint_in_service\").wait(\n",
    "            EndpointName=predictor.endpoint_name\n",
    "        )\n",
    "        print(\"Ready\")\n",
    "        return predictor\n",
    "    except WaiterError as err:\n",
    "        error_message = err.last_response[\"Error\"][\"Message\"]\n",
    "        if error_message.startswith(\"Could not find endpoint\"):\n",
    "            err = Exception(f\"Endpoint {endpoint_name} not found.\")\n",
    "        raise err\n",
    "\n",
    "\n",
    "predictor = wait_for_predictor(\"staging\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Staging\n",
    "\n",
    "Let's send some traffic to the staging endpoint with the following payload:\n",
    "\n",
    "| passenger_count\t| pickup_latitude\t| pickup_longitude\t| dropoff_latitude\t| dropoff_longitude\t| geo_distance\t| hour\t| weekday\t| month |\n",
    "| -| - | - | - | - | - | - | - | - |\n",
    "| 1\t| -73.986114\t| 40.685634\t| -73.936794\t| 40.715370\t| 5.318025\t| 7\t| 0\t| 2 |\n",
    "\n",
    "We expect approximately a $20 fare:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "payload = \"1,-73.986114,40.685634,-73.936794,40.715370,5.318025,7,0,2\"\n",
    "predictor.predict(data=payload)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Approve Staging\n",
    "\n",
    "ðŸ›‘ Click the link below to head over to the AWS Code Pipeline and approve the staging deployment to kick off the production deployment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(\n",
    "    f'Open <a target=\"_blank\" href=\"https://{region_name}.console.aws.amazon.com/codesuite/codepipeline/pipelines/sagemaker-{project_name}-deploy/view?region={region_name}\">Code Pipeline</a> in a new window'\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Production\n",
    "\n",
    "After a few minutes our production endpoint will start to be deployed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "predictor = wait_for_predictor(\"prod\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And confirm that data capture is enabled."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_capture = sm_client.describe_endpoint(EndpointName=predictor.endpoint_name)[\n",
    "    \"DataCaptureConfig\"\n",
    "]\n",
    "print(f\"Data capture is: {data_capture['CaptureStatus']}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspect Data Capture\n",
    "\n",
    "Let's send some traffic to the producition endpoint, which our [Data Quality Monitor](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-quality.html) should detect as drifting from the baseline."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "for n in range(100):\n",
    "    predictor.predict(data=payload)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see if we have received some outputs to our data capture"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_capture_uri = data_capture[\"DestinationS3Uri\"]\n",
    "data_capture_files = S3Downloader.list(data_capture_uri)\n",
    "\n",
    "print(\"Found {} files\".format(len(data_capture_files)))\n",
    "\n",
    "if data_capture[\"EnableCapture\"] and len(data_capture_files) > 0:\n",
    "    # Get the first line of the most recent file\n",
    "    event = json.loads(S3Downloader.read_file(data_capture_files[-1]).split(\"\\n\")[0])\n",
    "    print(\"\\nLast file:\\n{}\".format(json.dumps(event, indent=2)))\n",
    "elif len(data_capture_files) == 0:\n",
    "    print(\"No files yet, please rerun this cell in a few seconds\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's download the validation dataset from the latest processing job, and tweak some of the columns to change the distribution of the data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import random\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "\n",
    "def get_latest_processed_data():\n",
    "    execution_arn = sm_client.list_pipeline_executions(\n",
    "        PipelineName=pipeline_name, SortBy=\"CreationTime\"\n",
    "    )[\"PipelineExecutionSummaries\"][0][\"PipelineExecutionArn\"]\n",
    "    steps = sm_client.list_pipeline_execution_steps(\n",
    "        PipelineExecutionArn=execution_arn, SortOrder=\"Ascending\"\n",
    "    )[\"PipelineExecutionSteps\"]\n",
    "    preprocess_arn = next(\n",
    "        item[\"Metadata\"][\"ProcessingJob\"][\"Arn\"]\n",
    "        for item in steps\n",
    "        if item[\"StepName\"] == \"PreprocessData\"\n",
    "    )\n",
    "    job_outputs = sm_client.describe_processing_job(\n",
    "        ProcessingJobName=preprocess_arn.split(\"/\")[1]\n",
    "    )[\"ProcessingOutputConfig\"][\"Outputs\"]\n",
    "    validation_uri = next(\n",
    "        item[\"S3Output\"][\"S3Uri\"]\n",
    "        for item in job_outputs\n",
    "        if item[\"OutputName\"] == \"validation\"\n",
    "    )\n",
    "    return validation_uri\n",
    "\n",
    "\n",
    "dataset_location = get_latest_processed_data()\n",
    "S3Downloader().download(dataset_location, \"preprocessed\")\n",
    "df = pd.read_csv(\"preprocessed/validation.csv\", header=None)\n",
    "\n",
    "# Changing the distribution of data to artificially cause an alarm\n",
    "df[1] = random.choices([1, 2, 3, 4, 5, 6], weights=[2, 1, 2, 5, 2, 1], k=df.shape[0])\n",
    "df[6] = df[1].apply(lambda x: 70 * random.betavariate(2.5, 2))\n",
    "tweaked_rows = df.drop(0, axis=1).to_csv(header=False, index=False).split(\"\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then make a series of prediction requests in the background every 10 minutes with this  data to cause an artificial model monitoring alarm to be triggered."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from threading import Thread\n",
    "import time\n",
    "\n",
    "\n",
    "def invoke_endpoint_forever():\n",
    "    while True:\n",
    "        for i in range(10000):\n",
    "            predictor.predict(data=tweaked_rows[i % len(tweaked_rows)])\n",
    "        time.sleep(10 * 60)\n",
    "\n",
    "\n",
    "Thread(target=invoke_endpoint_forever).start()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The preceding code will start hitting the endpoint every 10 minutes with a load of randomly generated datapoints. These randomly generated data are created following a distribution that we know will trigger an alarm. This represents a simulation of reality where the distribution of the incoming data has changed due to changes in the environment. \n",
    "\n",
    "You can change the sleep time, the number of requests per batch, or the randomly generated data, to different values. This will allow you to test your endpoint with more requests per second or to see how different changes in data would affect your monitoring.  You can even completely remove the sleep time so that the kernel will be hitting the endpoint as fast as it can. However, this will cause the endpoint to work harder and trigger the automatic scaling to increase the underlying infrastructure used by the endpoint, which might incur higher costs. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Monitor\n",
    "\n",
    "Let's check that we have a monitor configured and that its schedule."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datetime import datetime, timedelta\n",
    "from dateutil.tz import tzlocal\n",
    "\n",
    "model_monitor = predictor.list_monitors()[0]\n",
    "model_monitor_status = model_monitor.describe_schedule()[\"MonitoringScheduleStatus\"]\n",
    "print(f\"Model Monitoring: {model_monitor_status}\")\n",
    "\n",
    "now = datetime.now(tzlocal())\n",
    "next_hour = (now + timedelta(hours=1)).replace(minute=0)\n",
    "scheduled_diff = (next_hour - now).seconds // 60\n",
    "print(\"Next schedule in {} minutes\".format(scheduled_diff))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "List the latest execution and output the status"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "monitor_executions = model_monitor.list_executions()\n",
    "if len(monitor_executions) == 0:\n",
    "    raise (Exception(\"Please wait, no monitor executions available yet\"))\n",
    "\n",
    "# Get the latest monitor status\n",
    "monitor_status = monitor_executions[0].describe()[\"ProcessingJobStatus\"]\n",
    "if monitor_status == \"Completed\":\n",
    "    monitor_message = monitor_executions[0].describe()[\"ExitMessage\"]\n",
    "    print(f\"Latest execution: {monitor_message}\")\n",
    "else:\n",
    "    print(f\"Latest execution: {monitor_status}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inspect Model Monitor report\n",
    "\n",
    "ðŸ›‘ Browse to the model monitoring results in SageMaker Studio to download and run a report"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Retrain\n",
    "\n",
    "When the model monitoring schedule runs it will publish Amazon [CloudWatch Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-interpreting-cloudwatch.html).  If drift is detected about a pre-configured threshold then an Amazon CloudWatch metric will Alarm resulting in the SageMaker pipeline to be re-trained.\n",
    "\n",
    "You can simulate drift by putting a metric value above the threshold of `0.5` directly into CloudWatch."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.tz import tzlocal\n",
    "import random\n",
    "\n",
    "\n",
    "cloudwatch = boto3.client(\"cloudwatch\")\n",
    "\n",
    "# Define the metric name and threshold\n",
    "endpoint_name = predictor.endpoint_name\n",
    "schedule_name = f\"{endpoint_name}-threshold\"\n",
    "metric_name = \"feature_baseline_drift_fare_amount\"\n",
    "metric_threshold = 0.5\n",
    "\n",
    "# Put a new metric to trigger an alaram\n",
    "def put_drift_metric(value):\n",
    "    print(\"Putting metric: {}\".format(value))\n",
    "    response = cloudwatch.put_metric_data(\n",
    "        Namespace=\"aws/sagemaker/Endpoints/data-metrics\",\n",
    "        MetricData=[\n",
    "            {\n",
    "                \"MetricName\": metric_name,\n",
    "                \"Dimensions\": [\n",
    "                    {\"Name\": \"MonitoringSchedule\", \"Value\": schedule_name},\n",
    "                    {\"Name\": \"Endpoint\", \"Value\": endpoint_name},\n",
    "                ],\n",
    "                \"Timestamp\": datetime.now(),\n",
    "                \"Value\": value,\n",
    "                \"Unit\": \"None\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "def get_drift_stats():\n",
    "    response = cloudwatch.get_metric_statistics(\n",
    "        Namespace=\"aws/sagemaker/Endpoints/data-metrics\",\n",
    "        MetricName=metric_name,\n",
    "        Dimensions=[\n",
    "            {\"Name\": \"MonitoringSchedule\", \"Value\": schedule_name},\n",
    "            {\"Name\": \"Endpoint\", \"Value\": endpoint_name},\n",
    "        ],\n",
    "        StartTime=datetime.now() - timedelta(minutes=2),\n",
    "        EndTime=datetime.now(),\n",
    "        Period=1,\n",
    "        Statistics=[\"Average\"],\n",
    "        Unit=\"None\",\n",
    "    )\n",
    "    if \"Datapoints\" in response and len(response[\"Datapoints\"]) > 0:\n",
    "        return response[\"Datapoints\"][0][\"Average\"]\n",
    "    return 0\n",
    "\n",
    "\n",
    "print(\"Simluate drift on endpoint: {}\".format(endpoint_name))\n",
    "\n",
    "while True:\n",
    "    put_drift_metric(round(random.uniform(metric_threshold, 1.0), 4))\n",
    "    drift_stats = get_drift_stats()\n",
    "    print(\"Average drift amount: {}\".format(get_drift_stats()))\n",
    "    if drift_stats > metric_threshold:\n",
    "        break\n",
    "    time.sleep(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To see the CloudWatch metric Alarm click on the link below."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "HTML(\n",
    "    f'Open <a target=\"_blank\" href=\"https://{region_name}.console.aws.amazon.com/cloudwatch/home?region={region_name}#alarmsV2:alarm/{schedule_name}\">CloudWatch Alarm</a> in new window'\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This will result in a new SageMaker pipeline execution starting."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "latest_pipeline_execution = sm_client.list_pipeline_executions(\n",
    "    PipelineName=pipeline_name,\n",
    ")[\"PipelineExecutionSummaries\"][0]\n",
    "latest_execution_status = latest_pipeline_execution[\"PipelineExecutionStatus\"]\n",
    "time_ago = datetime.now(tzlocal()) - latest_pipeline_execution[\"StartTime\"]\n",
    "\n",
    "print(\n",
    "    f\"Latest pipeline: {pipeline_name} execution: {latest_execution_status} started {time_ago.total_seconds()/60:0.2f} mins ago\"\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can verify that this was triggered by Drift by inspecting the InputSource:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params = sm_client.list_pipeline_parameters_for_execution(\n",
    "    PipelineExecutionArn=latest_pipeline_execution[\"PipelineExecutionArn\"],\n",
    ")\n",
    "input_source = [\n",
    "    p[\"Value\"] for p in params[\"PipelineParameters\"] if p[\"Name\"] == \"InputSource\"\n",
    "][0]\n",
    "print(f\"Pipeline execution started with InputSource: {input_source}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "And let's list the steps of that execution.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "execution_steps = sm_client.list_pipeline_execution_steps(\n",
    "    PipelineExecutionArn=latest_pipeline_execution[\"PipelineExecutionArn\"],\n",
    ")[\"PipelineExecutionSteps\"]\n",
    "for step in execution_steps:\n",
    "    print(\"Step: {}, Status: {}\".format(step[\"StepName\"], step[\"StepStatus\"]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "âœ… Great now you have completed all the steps."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Clean up\n",
    "\n",
    "Execute the following cell to delete any registered models."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "response = sm_client.list_model_packages(ModelPackageGroupName=project_name)\n",
    "for model_package in response[\"ModelPackageSummaryList\"]:\n",
    "    print(\"Deleting Version {}\".format(model_package[\"ModelPackageArn\"].split(\"/\")[-1]))\n",
    "    sm_client.delete_model_package(ModelPackageName=model_package[\"ModelPackageArn\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Execute the following cell to delete cloudformation stacks\n",
    "\n",
    "1. SageMaker prod endpoint\n",
    "2. SageMaker staging endpoint\n",
    "3. SageMaker Pipeline Workflow and Model Package Group"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import boto3\n",
    "\n",
    "cfn = boto3.client(\"cloudformation\")\n",
    "\n",
    "for stack_name in [\n",
    "    f\"sagemaker-{project_name}-deploy-prod\",\n",
    "    f\"sagemaker-{project_name}-deploy-staging\",\n",
    "    f\"sagemaker-{project_name}-pipeline\",\n",
    "]:\n",
    "    print(\"Deleting stack: {}\".format(stack_name))\n",
    "    cfn.delete_stack(StackName=stack_name)\n",
    "    cfn.get_waiter(\"stack_delete_complete\").wait(StackName=stack_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code will clean up all objects in the artifact bucket and delete the SageMaker project."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "s3_resource = boto3.resource(\"s3\")\n",
    "s3_artifact_bucket = s3_resource.Bucket(artifact_bucket)\n",
    "s3_artifact_bucket.object_versions.delete()\n",
    "print(\"Artifact bucket objects deleted\")\n",
    "\n",
    "sm_client.delete_project(ProjectName=project_name)\n",
    "print(\"SageMaker Project deleted\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "07c1d6c68b7b22b50965762993b154aa5a1dd6aa65a365988d7d4c27c573599b"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}