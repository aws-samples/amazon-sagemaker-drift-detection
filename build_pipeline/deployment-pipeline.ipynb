{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment Pipeline Notebook\n",
    "\n",
    "This notebook will exercise the drift detection MLOps `deployment pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Retrieve the project name from your build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get back the project id and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "\n",
    "sess = sagemaker.session.Session()\n",
    "region_name = sess._region_name\n",
    "sm_client = sess.sagemaker_client\n",
    "project_id = sm_client.describe_project(ProjectName=project_name)[\"ProjectId\"]\n",
    "artifact_bucket = f\"sagemaker-project-{project_id}-{region_name}\"\n",
    "\n",
    "print(f\"Project: {project_name} ({project_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your batch pipeline should now be running, click the link below to open the AWS CodePipeline in a new window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(\n",
    "    f'Open <a target=\"_blank\" href=\"https://{region_name}.console.aws.amazon.com/codesuite/codepipeline/pipelines/sagemaker-{project_name}-deploy/view?region={region_name}\">Code Pipeline</a> in a new window'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "\n",
    "Download the test dataset output from the pre-processing job in our build pipeline, which we will use for input to batch scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import random\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "\n",
    "\n",
    "def get_latest_processed_data(pipeline_name, step_name, output_name):\n",
    "    execution_arn = sm_client.list_pipeline_executions(\n",
    "        PipelineName=pipeline_name, SortBy=\"CreationTime\"\n",
    "    )[\"PipelineExecutionSummaries\"][0][\"PipelineExecutionArn\"]\n",
    "    steps = sm_client.list_pipeline_execution_steps(\n",
    "        PipelineExecutionArn=execution_arn, SortOrder=\"Ascending\"\n",
    "    )[\"PipelineExecutionSteps\"]\n",
    "    preprocess_arn = next(\n",
    "        item[\"Metadata\"][\"ProcessingJob\"][\"Arn\"]\n",
    "        for item in steps\n",
    "        if item[\"StepName\"] == step_name\n",
    "    )\n",
    "    job_outputs = sm_client.describe_processing_job(\n",
    "        ProcessingJobName=preprocess_arn.split(\"/\")[1]\n",
    "    )[\"ProcessingOutputConfig\"][\"Outputs\"]\n",
    "    return next(\n",
    "        item[\"S3Output\"][\"S3Uri\"]\n",
    "        for item in job_outputs\n",
    "        if item[\"OutputName\"] == output_name\n",
    "    )\n",
    "\n",
    "\n",
    "pipeline_name = f\"{project_name}-build\"\n",
    "test_uri = get_latest_processed_data(pipeline_name, \"PreprocessData\", \"test\")\n",
    "S3Downloader().download(test_uri, \"preprocessed\")\n",
    "\n",
    "# Load the test scores into a dataframe\n",
    "test_df = pd.read_csv(\"preprocessed/test.csv\")\n",
    "print(test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Staging\n",
    "\n",
    "The staging SageMaker endpoint is created by AWS CloudFormation in the `Batch_CFN_Staging` stage of the AWS CodePipeline\n",
    "\n",
    "Once its created, run the next cell to wait for the staging endpoint to be in service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import WaiterError\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Define the predictor for staging\n",
    "def wait_for_predictor(stage_name):\n",
    "    try:\n",
    "        endpoint_name = f\"sagemaker-{project_name}-{stage_name}\"\n",
    "        predictor = Predictor(\n",
    "            endpoint_name, serializer=CSVSerializer(), deserializer=JSONDeserializer()\n",
    "        )\n",
    "        print(\n",
    "            f\"Waiting for {stage_name} endpoint: {predictor.endpoint_name} to be deployed...\"\n",
    "        )\n",
    "        sm_client.get_waiter(\"endpoint_in_service\").wait(\n",
    "            EndpointName=predictor.endpoint_name\n",
    "        )\n",
    "        print(\"Ready\")\n",
    "        return predictor\n",
    "    except WaiterError as err:\n",
    "        error_message = err.last_response[\"Error\"][\"Message\"]\n",
    "        if error_message.startswith(\"Could not find endpoint\"):\n",
    "            err = Exception(f\"Endpoint {endpoint_name} not found.\")\n",
    "        raise err\n",
    "\n",
    "\n",
    "predictor = wait_for_predictor(\"staging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's send some traffic to the staging endpoint with the following payload:\n",
    "\n",
    "| passenger_count\t| pickup_latitude\t| pickup_longitude\t| dropoff_latitude\t| dropoff_longitude\t| geo_distance\t| hour\t| weekday\t| month |\n",
    "| -| - | - | - | - | - | - | - | - |\n",
    "| 1\t| -73.986114\t| 40.685634\t| -73.936794\t| 40.715370\t| 5.318025\t| 7\t| 0\t| 2 |\n",
    "\n",
    "We expect approximately a $20 fare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = \"1,-73.986114,40.685634,-73.936794,40.715370,5.318025,7,0,2\"\n",
    "predictor.predict(data=payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approve Staging\n",
    "\n",
    "ðŸ›‘  Head back to the AWS Code Pipeline and approve the staging deployment to kick off the production deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Production\n",
    "\n",
    "After a few minutes our production endpoint will start to be deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = wait_for_predictor(\"prod\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And confirm that data capture is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_capture = sm_client.describe_endpoint(EndpointName=predictor.endpoint_name)[\n",
    "    \"DataCaptureConfig\"\n",
    "]\n",
    "print(f\"Data capture is: {data_capture['CaptureStatus']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Data Capture\n",
    "\n",
    "Let's send some traffic to the producition endpoint, which our [Data Quality Monitor](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-data-quality.html) should detect as drifting from the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for n in range(100):\n",
    "    predictor.predict(data=payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we have received some outputs to our data capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_capture_uri = data_capture[\"DestinationS3Uri\"]\n",
    "data_capture_files = S3Downloader.list(data_capture_uri)\n",
    "\n",
    "print(\"Found {} files\".format(len(data_capture_files)))\n",
    "\n",
    "if data_capture[\"EnableCapture\"] and len(data_capture_files) > 0:\n",
    "    # Get the first line of the most recent file\n",
    "    event = json.loads(S3Downloader.read_file(data_capture_files[-1]).split(\"\\n\")[0])\n",
    "    print(\"\\nLast file:\\n{}\".format(json.dumps(event, indent=2)))\n",
    "elif len(data_capture_files) == 0:\n",
    "    print(\"No files yet, please rerun this cell in a few seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we test production, let's tweak some of the columns to change the distribution of the data. \n",
    "\n",
    "This represents a simulation of reality where the distribution of the incoming data has changed due to changes in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"passenger_count\"] = random.choices(\n",
    "    [1, 2, 3, 4, 5, 6], weights=[2, 1, 2, 5, 2, 1], k=test_df.shape[0]\n",
    ")\n",
    "test_df[\"geo_distance\"] = test_df[\"passenger_count\"].apply(\n",
    "    lambda x: 70 * random.betavariate(2.5, 2)\n",
    ")\n",
    "\n",
    "tweaked_rows = (\n",
    "    test_df.drop(\"fare_amount\", axis=1).to_csv(header=False, index=False).split(\"\\n\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then make a series of prediction requests in the background every 10 minutes with this  data to cause an artificial model monitoring alarm to be triggered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import time\n",
    "\n",
    "\n",
    "def invoke_endpoint_forever():\n",
    "    while True:\n",
    "        for i in range(10000):\n",
    "            predictor.predict(data=tweaked_rows[i % len(tweaked_rows)])\n",
    "        time.sleep(10 * 60)\n",
    "\n",
    "\n",
    "Thread(target=invoke_endpoint_forever).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code can change the sleep time, the number of requests per batch, or the randomly generated data, to different values. This will allow you to test your endpoint with more requests per second or to see how different changes in data would affect your monitoring.  You can even completely remove the sleep time so that the kernel will be hitting the endpoint as fast as it can. However, this will cause the endpoint to work harder and trigger the automatic scaling to increase the underlying infrastructure used by the endpoint, which might incur higher costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor\n",
    "\n",
    "Let's check that we have a monitor configured and that its schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from dateutil.tz import tzlocal\n",
    "\n",
    "model_monitor = predictor.list_monitors()[0]\n",
    "model_monitor_status = model_monitor.describe_schedule()[\"MonitoringScheduleStatus\"]\n",
    "print(f\"Model Monitoring: {model_monitor_status}\")\n",
    "\n",
    "now = datetime.now(tzlocal())\n",
    "next_hour = (now + timedelta(hours=1)).replace(minute=0)\n",
    "scheduled_diff = (next_hour - now).seconds // 60\n",
    "print(\"Next schedule in {} minutes\".format(scheduled_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the latest execution and output the status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_executions = model_monitor.list_executions()\n",
    "if len(monitor_executions) == 0:\n",
    "    raise (Exception(\"Please wait, no monitor executions available yet\"))\n",
    "\n",
    "# Get the latest monitor status\n",
    "monitor_status = monitor_executions[0].describe()[\"ProcessingJobStatus\"]\n",
    "if monitor_status == \"Completed\":\n",
    "    monitor_message = monitor_executions[0].describe()[\"ExitMessage\"]\n",
    "    print(f\"Latest execution: {monitor_message}\")\n",
    "else:\n",
    "    print(f\"Latest execution: {monitor_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Model Monitor report\n",
    "\n",
    "ðŸ›‘ Browse to the model monitoring results in SageMaker Studio to download and run a report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain\n",
    "\n",
    "When the model monitoring schedule runs it will publish Amazon [CloudWatch Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-interpreting-cloudwatch.html).  \n",
    "\n",
    "If drift is detected for a metric above the threshold defined in the `prod-config.json` in the deployment pipeline, then the Amazon CloudWatch will Alarm resulting in the SageMaker pipeline to be re-trained.\n",
    "\n",
    "You can simulate drift by putting a metric value above the threshold of `0.5` directly into CloudWatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.tz import tzlocal\n",
    "import random\n",
    "\n",
    "\n",
    "cloudwatch = boto3.client(\"cloudwatch\")\n",
    "\n",
    "# Define the metric name and threshold\n",
    "endpoint_name = predictor.endpoint_name\n",
    "schedule_name = f\"{endpoint_name}-threshold\"\n",
    "metric_name = \"feature_baseline_drift_fare_amount\"\n",
    "metric_threshold = 0.5\n",
    "\n",
    "# Put a new metric to trigger an alaram\n",
    "def put_drift_metric(value):\n",
    "    print(\"Putting metric: {}\".format(value))\n",
    "    response = cloudwatch.put_metric_data(\n",
    "        Namespace=\"aws/sagemaker/Endpoints/data-metrics\",\n",
    "        MetricData=[\n",
    "            {\n",
    "                \"MetricName\": metric_name,\n",
    "                \"Dimensions\": [\n",
    "                    {\"Name\": \"MonitoringSchedule\", \"Value\": schedule_name},\n",
    "                    {\"Name\": \"Endpoint\", \"Value\": endpoint_name},\n",
    "                ],\n",
    "                \"Timestamp\": datetime.now(),\n",
    "                \"Value\": value,\n",
    "                \"Unit\": \"None\",\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "def get_drift_stats():\n",
    "    response = cloudwatch.get_metric_statistics(\n",
    "        Namespace=\"aws/sagemaker/Endpoints/data-metrics\",\n",
    "        MetricName=metric_name,\n",
    "        Dimensions=[\n",
    "            {\"Name\": \"MonitoringSchedule\", \"Value\": schedule_name},\n",
    "            {\"Name\": \"Endpoint\", \"Value\": endpoint_name},\n",
    "        ],\n",
    "        StartTime=datetime.now() - timedelta(minutes=2),\n",
    "        EndTime=datetime.now(),\n",
    "        Period=1,\n",
    "        Statistics=[\"Average\"],\n",
    "        Unit=\"None\",\n",
    "    )\n",
    "    if \"Datapoints\" in response and len(response[\"Datapoints\"]) > 0:\n",
    "        return response[\"Datapoints\"][0][\"Average\"]\n",
    "    return 0\n",
    "\n",
    "\n",
    "print(\"Simluate drift on endpoint: {}\".format(endpoint_name))\n",
    "\n",
    "while True:\n",
    "    put_drift_metric(round(random.uniform(metric_threshold, 1.0), 4))\n",
    "    drift_stats = get_drift_stats()\n",
    "    print(\"Average drift amount: {}\".format(get_drift_stats()))\n",
    "    if drift_stats > metric_threshold:\n",
    "        break\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the CloudWatch metric Alarm click on the link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\n",
    "    f'Open <a target=\"_blank\" href=\"https://{region_name}.console.aws.amazon.com/cloudwatch/home?region={region_name}#alarmsV2:alarm/{schedule_name}\">CloudWatch Alarm</a> in new window'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will result in a new SageMaker pipeline execution starting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_pipeline_execution = sm_client.list_pipeline_executions(\n",
    "    PipelineName=pipeline_name,\n",
    ")[\"PipelineExecutionSummaries\"][0]\n",
    "latest_execution_status = latest_pipeline_execution[\"PipelineExecutionStatus\"]\n",
    "time_ago = datetime.now(tzlocal()) - latest_pipeline_execution[\"StartTime\"]\n",
    "\n",
    "print(\n",
    "    f\"Latest pipeline: {pipeline_name} execution: {latest_execution_status} started {time_ago.total_seconds()/60:0.2f} mins ago\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that this was triggered by Drift by inspecting the InputSource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = sm_client.list_pipeline_parameters_for_execution(\n",
    "    PipelineExecutionArn=latest_pipeline_execution[\"PipelineExecutionArn\"],\n",
    ")\n",
    "input_source = [\n",
    "    p[\"Value\"] for p in params[\"PipelineParameters\"] if p[\"Name\"] == \"InputSource\"\n",
    "][0]\n",
    "print(f\"Pipeline execution started with InputSource: {input_source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's list the steps of that execution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_steps = sm_client.list_pipeline_execution_steps(\n",
    "    PipelineExecutionArn=latest_pipeline_execution[\"PipelineExecutionArn\"],\n",
    ")[\"PipelineExecutionSteps\"]\n",
    "for step in execution_steps:\n",
    "    print(\"Step: {}, Status: {}\".format(step[\"StepName\"], step[\"StepStatus\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… Great now you have completed all the steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Execute the following cell to delete cloudformation stacks\n",
    "\n",
    "1. SageMaker prod endpoint\n",
    "2. SageMaker staging endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "cfn = boto3.client(\"cloudformation\")\n",
    "\n",
    "for stack_name in [\n",
    "    f\"sagemaker-{project_name}-deploy-prod\",\n",
    "    f\"sagemaker-{project_name}-deploy-staging\",\n",
    "]:\n",
    "    print(\"Deleting stack: {}\".format(stack_name))\n",
    "    cfn.delete_stack(StackName=stack_name)\n",
    "    cfn.get_waiter(\"stack_delete_complete\").wait(StackName=stack_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can return to the [build-pipeline](build-pipeline.ipynb) notebook to complete the cleanup."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "07c1d6c68b7b22b50965762993b154aa5a1dd6aa65a365988d7d4c27c573599b"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-1:742091327244:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
