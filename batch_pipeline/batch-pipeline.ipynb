{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Pipeline Notebook\n",
    "\n",
    "This notebook will exercise the drift detection MLOps batch pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "ðŸ‘‡ Set the project name for your drift pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"<<project_name>>\"  # << Update this drift detection project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get back the project id and region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import json\n",
    "\n",
    "sess = sagemaker.session.Session()\n",
    "region_name = sess._region_name\n",
    "sm_client = sess.sagemaker_client\n",
    "project_id = sm_client.describe_project(ProjectName=project_name)[\"ProjectId\"]\n",
    "artifact_bucket = f\"sagemaker-project-{project_id}-{region_name}\"\n",
    "\n",
    "print(f\"Project: {project_name} ({project_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "\n",
    "Download the test dataset output from the pre-processing job in our build pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import random\n",
    "from sagemaker.s3 import S3Downloader, S3Uploader\n",
    "\n",
    "\n",
    "def get_latest_processed_data(pipeline_name, step_name, output_name):\n",
    "    execution_arn = sm_client.list_pipeline_executions(\n",
    "        PipelineName=pipeline_name, SortBy=\"CreationTime\"\n",
    "    )[\"PipelineExecutionSummaries\"][0][\"PipelineExecutionArn\"]\n",
    "    steps = sm_client.list_pipeline_execution_steps(\n",
    "        PipelineExecutionArn=execution_arn, SortOrder=\"Ascending\"\n",
    "    )[\"PipelineExecutionSteps\"]\n",
    "    preprocess_arn = next(\n",
    "        item[\"Metadata\"][\"ProcessingJob\"][\"Arn\"]\n",
    "        for item in steps\n",
    "        if item[\"StepName\"] == step_name\n",
    "    )\n",
    "    job_outputs = sm_client.describe_processing_job(\n",
    "        ProcessingJobName=preprocess_arn.split(\"/\")[1]\n",
    "    )[\"ProcessingOutputConfig\"][\"Outputs\"]\n",
    "    return next(\n",
    "        item[\"S3Output\"][\"S3Uri\"]\n",
    "        for item in job_outputs\n",
    "        if item[\"OutputName\"] == output_name\n",
    "    )\n",
    "\n",
    "\n",
    "pipeline_name = f\"{project_name}-build\"\n",
    "test_uri = get_latest_processed_data(pipeline_name, \"PreprocessData\", \"test\")\n",
    "S3Downloader().download(test_uri, \"preprocessed\")\n",
    "\n",
    "# Load the test scores into a dataframe\n",
    "test_df = pd.read_csv(\"preprocessed/test.csv\")\n",
    "print(test_df.shape)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the test dataset to the batch staging input location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_staging_uri = f\"s3://{artifact_bucket}/{project_id}/batch/staging\"\n",
    "S3Uploader().upload(\"preprocessed/test.csv\", batch_staging_uri);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Staging\n",
    "\n",
    "Now let's start the batch staging pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = f\"{project_name}-batch-staging\"\n",
    "pipeline = Pipeline(pipeline_name)\n",
    "\n",
    "# Start pipeline\n",
    "execution = pipeline.start()\n",
    "execution_name = execution.arn.split(\"/\")[-1]\n",
    "\n",
    "print(f\"Waiting for execution: {execution_name} for pipeline {pipeline_name}...\")\n",
    "execution.wait()\n",
    "execution_status = execution.describe()[\"PipelineExecutionStatus\"]\n",
    "print(f\"Status: {execution_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this has completed, download the batch scoring results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_scores_uri = get_latest_processed_data(pipeline_name, \"ScoreModel\", \"scores\")\n",
    "S3Downloader().download(staging_scores_uri, \"staging\")\n",
    "\n",
    "# Load the predicted scores, and join with test dataframe\n",
    "pred_df = pd.read_csv(\"staging/scores.csv\")\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Calculate the root mean squre error (RMSE) to evaluate the performance of this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "mse = mean_squared_error(test_df[\"fare_amount\"], pred_df[\"fare_amount_prediction\"])\n",
    "rmse = sqrt(mse)\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the residules to see where the errors are relative to the fare amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.residplot(\n",
    "    x=pred_df[\"fare_amount\"], y=pred_df[\"fare_amount_prediction\"], lowess=True\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approve Staging\n",
    "\n",
    "ðŸ›‘ Click the link below to head over to the AWS Code Pipeline and approve the staging batch scoring to kick off the production batch scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(\n",
    "    f'Open <a target=\"_blank\" href=\"https://{region_name}.console.aws.amazon.com/codesuite/codepipeline/pipelines/sagemaker-{project_name}-batch/view?region={region_name}\">Code Pipeline</a> in a new window'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Production\n",
    "\n",
    "Before we test production, it let's tweak some of the columns to change the distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"passenger_count\"] = random.choices(\n",
    "    [1, 2, 3, 4, 5, 6], weights=[2, 1, 2, 5, 2, 1], k=test_df.shape[0]\n",
    ")\n",
    "test_df[\"geo_distance\"] = test_df[\"passenger_count\"].apply(\n",
    "    lambda x: 70 * random.betavariate(2.5, 2)\n",
    ")\n",
    "\n",
    "test_df.to_csv(\"preprocessed/tweaked.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the tweaked dataset to the production input location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prod_uri = f\"s3://{artifact_bucket}/{project_id}/batch/prod\"\n",
    "S3Uploader().upload(\"preprocessed/tweaked.csv\", batch_prod_uri);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few minutes our production batch pipeline will be ready for scoring.   \n",
    "\n",
    "Start the production batch pipeline and wait for it to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = f\"{project_name}-batch-prod\"\n",
    "pipeline = Pipeline(pipeline_name)\n",
    "\n",
    "# Start pipeline\n",
    "execution = pipeline.start()\n",
    "execution_name = execution.arn.split(\"/\")[-1]\n",
    "\n",
    "print(f\"Waiting for execution: {execution_name} for pipeline {pipeline_name}...\")\n",
    "execution.wait()\n",
    "execution_status = execution.describe()[\"PipelineExecutionStatus\"]\n",
    "print(f\"Status: {execution_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list steps, and we will see the last step was to `EvaluateDrift` Lambda function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in execution.list_steps():\n",
    "    print(\"Step: {}, Status: {}\".format(step[\"StepName\"], step[\"StepStatus\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor\n",
    "\n",
    "Let's let the files produced by the Model Monitor job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor_uri = get_latest_processed_data(\n",
    "    pipeline_name, \"ModelMonitor\", \"monitoring_output\"\n",
    ")\n",
    "\n",
    "print(\"Downloading monitor files:\")\n",
    "for s3_uri in S3Downloader.list(monitor_uri):\n",
    "    print(s3_uri.split(\"/\")[-1])\n",
    "\n",
    "S3Downloader().download(monitor_uri, \"monitor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the job has produced any `constraint_violations.json` let's output this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "violations = None\n",
    "if \"constraint_violations.json\" in os.listdir(\"monitor\"):\n",
    "    with open(\"monitor/constraint_violations.json\", \"r\") as f:\n",
    "        violations = json.load(f)[\"violations\"]\n",
    "else:\n",
    "    print(\"No violations\")\n",
    "\n",
    "violations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain\n",
    "\n",
    "The `EvaluateDrift` Lambda will read the contents of `constraint_violations.json` and will publish Amazon [CloudWatch Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-interpreting-cloudwatch.html).  \n",
    "\n",
    "If drift is detected above threshold for the target metric then a Amazon CloudWatch metric will Alarm resulting in the SageMaker pipeline to be re-trained.\n",
    "\n",
    "To see the CloudWatch metric Alarm click on the link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alarm_name = f\"sagemaker-{pipeline_name}-threshold\"\n",
    "\n",
    "HTML(\n",
    "    f'Open <a target=\"_blank\" href=\"https://{region_name}.console.aws.amazon.com/cloudwatch/home?region={region_name}#alarmsV2:alarm/{alarm_name}\">CloudWatch Alarm</a> in new window'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will result in a new SageMaker pipeline execution starting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from dateutil.tz import tzlocal\n",
    "\n",
    "pipeline_name = f\"{project_name}-build\"\n",
    "\n",
    "latest_pipeline_execution = sm_client.list_pipeline_executions(\n",
    "    PipelineName=pipeline_name,\n",
    ")[\"PipelineExecutionSummaries\"][0]\n",
    "latest_execution_status = latest_pipeline_execution[\"PipelineExecutionStatus\"]\n",
    "time_ago = datetime.now(tzlocal()) - latest_pipeline_execution[\"StartTime\"]\n",
    "\n",
    "print(\n",
    "    f\"Latest pipeline: {pipeline_name} execution: {latest_execution_status} started {time_ago.total_seconds()/60:0.2f} mins ago\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that this was triggered by Drift by inspecting the InputSource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = sm_client.list_pipeline_parameters_for_execution(\n",
    "    PipelineExecutionArn=latest_pipeline_execution[\"PipelineExecutionArn\"],\n",
    ")\n",
    "input_source = [\n",
    "    p[\"Value\"] for p in params[\"PipelineParameters\"] if p[\"Name\"] == \"InputSource\"\n",
    "][0]\n",
    "print(f\"Pipeline execution started with InputSource: {input_source}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's list the steps of that execution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_steps = sm_client.list_pipeline_execution_steps(\n",
    "    PipelineExecutionArn=latest_pipeline_execution[\"PipelineExecutionArn\"],\n",
    ")[\"PipelineExecutionSteps\"]\n",
    "for step in execution_steps:\n",
    "    print(\"Step: {}, Status: {}\".format(step[\"StepName\"], step[\"StepStatus\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… Great now you have completed all the steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Execute the following cell to delete cloudformation stacks\n",
    "\n",
    "1. SageMaker batch prod pipeline\n",
    "2. SageMaker batch staging pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "cfn = boto3.client(\"cloudformation\")\n",
    "\n",
    "for stack_name in [\n",
    "    f\"sagemaker-{project_name}-batch-prod\",\n",
    "    f\"sagemaker-{project_name}-batch-staging\",\n",
    "]:\n",
    "    print(\"Deleting stack: {}\".format(stack_name))\n",
    "    cfn.delete_stack(StackName=stack_name)\n",
    "    cfn.get_waiter(\"stack_delete_complete\").wait(StackName=stack_name)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "07c1d6c68b7b22b50965762993b154aa5a1dd6aa65a365988d7d4c27c573599b"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-1:742091327244:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
